# Performance Optimization Plan

**Date:** 2025-02-06
**Branch:** cleanup
**OpenCV:** 4.12.0 (crate 0.94)
**Current parallelism:** Rayon + OpenCL (UMat)

---

## Executive Summary

The application uses Rayon for multithreading and OpenCL (via UMat) for GPU acceleration, but a **global `OPENCL_MUTEX`** serializes most GPU work, negating the parallelism in several critical paths. Removing this mutex is the single highest-impact optimization. Additionally, the stacking pipeline processes images sequentially where parallel pyramid generation is possible, and the warping phase in ORB/SIFT/AKAZE/ECC alignment uses CPU `Mat` instead of GPU `UMat`.

---

## 1. Sharpness Detection

**Files:** `src/sharpness.rs` (464 lines), `src/gui/handlers/sharpness_handlers.rs` (178 lines)

### Current State

| Aspect | Status | Details |
|--------|--------|---------|
| GPU ops | ‚úÖ Full | `compute_sharpness_umat` / `compute_regional_sharpness_umat` ‚Äî cvtColor, GaussianBlur, Laplacian, Sobel, multiply, add all on UMat |
| Rayon parallelism | ‚úÖ Used | `sharpness_handlers.rs:99` ‚Äî `images.par_iter()` |
| OPENCL_MUTEX | ‚ùå **Bottleneck** | `sharpness_handlers.rs:116` ‚Äî wraps entire `compute_regional_sharpness_auto` call, serializing ALL GPU sharpness work despite `par_iter` |

### Bottleneck Detail

```text
sharpness_handlers.rs:114-117:
    static OPENCL_MUTEX: Mutex<()> = Mutex::new(());
    let _lock = OPENCL_MUTEX.lock().unwrap();
    let (max_regional, global_sharpness, sharp_region_count) =
        crate::sharpness::compute_regional_sharpness_auto(&img, ...)?;
    drop(_lock);
```

This means: N Rayon threads load N images in parallel, then queue up single-file for GPU sharpness computation. The parallelism is completely wasted.

### Optimizations

| ID | Change | Impact | Effort | Risk |
|----|--------|--------|--------|------|
| **S1** | **Remove OPENCL_MUTEX from sharpness handler** ‚Äî OpenCV 4.12 handles OpenCL thread safety internally. The ECC code already runs without mutex (`IMAGESTACKER_ECC_MUTEX=0`). Apply the same approach here. | üî• High ‚Äî enables true parallel GPU sharpness across all Rayon threads | Easy | Low ‚Äî same OpenCV version already works mutex-free in ECC |
| **S2** | **Parallel regional grid** ‚Äî `compute_regional_sharpness_umat` processes grid cells sequentially (nested for-loop, lines 420-440). With 16√ó16 grid = 256 sequential GPU calls per image. Could batch ROI extraction or parallelize regions. | Medium ‚Äî reduces per-image sharpness time | Medium | Low |
| **S3** | **Eliminate redundant global sharpness call** ‚Äî `compute_regional_sharpness_umat` computes full-image sharpness via separate `compute_sharpness_umat(img_umat)` call (line 451) in addition to all regions. The global score could be derived from the mean of regional scores instead. | Low ‚Äî saves one full-image GPU pass per image | Easy | None |

---

## 2. Thumbnail Generation

**Files:** `src/thumbnail.rs` (80 lines), `src/gui/handlers/file_handlers.rs` (417 lines)

### Current State

| Aspect | Status | Details |
|--------|--------|---------|
| GPU ops | ‚úÖ Partial | UMat used only for `imgproc::resize` (thumbnail.rs:58-65) |
| Rayon parallelism | ‚úÖ Used | `file_handlers.rs:200,404` ‚Äî `paths.par_iter()` |
| OPENCL_MUTEX | ‚úÖ None | Thumbnails already run freely in parallel (good!) |

### Analysis

The thumbnail pipeline is: `imread` (CPU) ‚Üí `get_umat` (upload) ‚Üí `resize` (GPU) ‚Üí `get_mat` (download). For an 800px target, the GPU upload/download overhead likely dominates the actual resize computation. CPU `resize` with `INTER_AREA` would be simpler and possibly faster at this scale.

### Optimizations

| ID | Change | Impact | Effort | Risk |
|----|--------|--------|--------|------|
| **T1** | **CPU-only resize for thumbnails** ‚Äî Target is only 800px max. Remove UMat upload/download overhead. Use CPU `Mat` `imgproc::resize` directly. | Medium ‚Äî less overhead per thumbnail, simpler code | Easy | None |
| **T2** | **Lazy/progressive thumbnails** ‚Äî Generate fast 200px thumbnails (INTER_NEAREST) immediately for responsive UI, then upgrade to 800px (INTER_AREA) in background. | Medium ‚Äî perceived speed improvement | Medium | Low |

---

## 3. Alignment

**File:** `src/alignment.rs` (2101 lines)

### 3a. ORB / SIFT / AKAZE (Feature-Based) ‚Äî `align_images()` line 1083+

#### Current State

| Phase | GPU | Parallel | Mutex | Lines |
|-------|-----|----------|-------|-------|
| Sharpness pre-filter | ‚úÖ UMat | ‚úÖ par_iter (batched) | ‚ùå OPENCL_MUTEX (line 1203) | 1140-1260 |
| GPU preprocessing (cvtColor, CLAHE, resize) | ‚úÖ UMat | ‚úÖ par_iter | ‚ùå **OPENCL_MUTEX (line 1508)** | 1494-1580 |
| Feature detection (detect_and_compute) | ‚ùå CPU Mat | ‚úÖ par_iter (after GPU unlock) | ‚úÖ None needed | 1580-1590 |
| Pairwise matching | ‚ùå CPU | ‚ùå Sequential | N/A | 1650-1780 |
| Warping | ‚ùå **CPU Mat** | ‚úÖ par_iter | ‚ùå **OPENCL_MUTEX (line 1934)** | 1890-1960 |

#### Bottleneck Detail: GPU Preprocessing

```text
alignment.rs:1504-1510:
    let (preprocessed, scale) = {
        let _lock = opencl_mutex().lock().unwrap();
        let img_umat = img.get_umat(...)?;
        // ... cvtColor, CLAHE, resize all on GPU ...
        let small_img = small_umat.get_mat(...)?;
    };
    // Feature detection runs AFTER lock is released ‚Äî truly parallel
```

The GPU preprocessing (upload ‚Üí cvtColor ‚Üí CLAHE ‚Üí resize ‚Üí download) is serialized. Each thread waits for the previous thread's GPU work to finish. Feature detection (CPU) runs in parallel after the lock is released ‚Äî this part is fine.

#### Bottleneck Detail: Warping

```text
alignment.rs:1930-1945:
    let (warped, output_path) = {
        let _lock = opencl_mutex().lock().unwrap();
        // ... cvtColor, warp_affine, mask operations ...
    };
```

All warping is serialized AND uses CPU `Mat` instead of GPU `UMat`.

#### Optimizations

| ID | Change | Impact | Effort | Risk |
|----|--------|--------|--------|------|
| **A1** | **Remove OPENCL_MUTEX from GPU preprocessing** ‚Äî OpenCV 4.12 handles OpenCL thread safety internally. This is the biggest single bottleneck for ORB/SIFT/AKAZE. | üî• High ‚Äî true parallel GPU preprocessing for all Rayon threads | Easy | Low ‚Äî test with 2-3 threads first |
| **A2** | **Remove OPENCL_MUTEX from warping** ‚Äî `warp_affine` should be thread-safe in OpenCV 4.12. | üî• High ‚Äî true parallel warping | Easy | Low |
| **A3** | **Use UMat for warping** ‚Äî Currently warps on CPU `Mat` (lines 1938-1945). Convert input to UMat ‚Üí GPU warp_affine ‚Üí download for imwrite. For 42MP images, GPU warp is significantly faster. | High ‚Äî GPU warp >> CPU warp at 42MP | Medium | Low |
| **A4** | **Parallel pairwise matching** ‚Äî Feature matching (lines 1650-1780) runs sequentially for consecutive pairs. Pairs are independent and could be parallelized. | Medium ‚Äî faster matching phase | Medium | Low |

### 3b. ECC ‚Äî `align_images_ecc()` line 644+

#### Current State

| Phase | GPU | Parallel | Mutex | Lines |
|-------|-----|----------|-------|-------|
| Sharpness pre-filter | ‚úÖ UMat (auto) | ‚ùå Sequential (per-image loop) | ‚úÖ opencl_mutex (line 712) | 690-760 |
| Preprocessing (cvtColor, GaussianBlur) | ‚ùå CPU Mat | ‚úÖ par_iter (per batch) | ‚úÖ None | 890-905 |
| ECC transform (`find_transform_ecc`) | ‚ùå CPU (internally may use OpenCL) | ‚úÖ par_iter | ‚úÖ **No mutex by default** (`IMAGESTACKER_ECC_MUTEX=0`) | 908-920 |
| Warping | ‚ùå **CPU Mat** | ‚úÖ par_iter | ‚úÖ None | 1020-1043 |
| File I/O (imwrite) | ‚ùå CPU | ‚úÖ par_iter | ‚úÖ None | 1053-1055 |

ECC is the best-parallelized algorithm ‚Äî `find_transform_ecc` already runs without mutex. The main gap is CPU-only warping.

#### Optimizations

| ID | Change | Impact | Effort | Risk |
|----|--------|--------|--------|------|
| **E1** | **GPU warping for ECC** ‚Äî Use UMat for `warp_perspective`/`warp_affine` (lines 1020-1043). Currently CPU `Mat`. | High ‚Äî GPU warp for 42MP images | Medium | Low |
| **E2** | **Parallel sharpness pre-filter** ‚Äî ECC sharpness computation (lines 690-760) is sequential (for-loop), unlike ORB/SIFT/AKAZE which use par_iter. Add par_iter with batching. | Medium ‚Äî faster sharpness phase for ECC | Easy | Low |
| **E3** | **GPU preprocessing** ‚Äî ECC preprocessing (cvtColor, GaussianBlur) uses CPU `Mat`. Convert to UMat pipeline for GPU acceleration. | Medium ‚Äî benefits large images | Medium | Low |

### 3c. ECC-Hybrid ‚Äî `compute_hybrid_ecc_transform()` line 233+

#### Current State

| Phase | GPU | Parallel | Mutex | Lines |
|-------|-----|----------|-------|-------|
| Keypoint extraction (SIFT) | ‚ùå CPU | N/A (per-image) | ‚ùå **OPENCL_MUTEX (line 248)** | 246-255 |
| Feature matching | ‚ùå CPU | N/A (per-image) | ‚úÖ None | 260-340 |
| ECC refinement | ‚ùå CPU (may use OpenCL internally) | N/A (per-image) | ‚úÖ No mutex (follows ECC_MUTEX env) | 370-395 |

#### Optimizations

| ID | Change | Impact | Effort | Risk |
|----|--------|--------|--------|------|
| **H1** | **Remove OPENCL_MUTEX from keypoint extraction** ‚Äî Line 248 locks mutex around `extract_features(ref_img, FeatureDetector::SIFT)`. SIFT `detect_and_compute` runs on CPU `Mat` ‚Äî no GPU mutex needed. | Medium ‚Äî faster hybrid init in parallel batches | Easy | None ‚Äî pure CPU operation |

---

## 4. Stacking

**File:** `src/stacking.rs` (602 lines)

### Current State

| Phase | GPU | Parallel | Lines |
|-------|-----|----------|-------|
| Image loading | ‚ùå CPU | ‚úÖ par_iter (line 119) | 115-130 |
| BGR/Alpha extraction | ‚úÖ UMat | ‚ùå Sequential (per-image in loop) | 247-260 |
| Laplacian pyramid generation | ‚úÖ UMat (pyr_down, pyr_up, subtract) | ‚ùå **Sequential** (per-image in loop) | 262 |
| Sharpness energy computation | ‚úÖ UMat (Laplacian, absdiff, GaussianBlur) | ‚ùå Sequential | 268-280 |
| Layer fusion (winner-take-all) | ‚úÖ UMat (compare, copy_to_masked) | ‚ùå Sequential (inherently ‚Äî depends on previous) | 282-300 |
| Pyramid collapse | ‚úÖ UMat (pyr_up, add, clip) | ‚ùå Sequential (inherently ‚Äî level by level) | 565-600 |
| Alpha assembly + erosion | ‚úÖ UMat | N/A | 460-510 |

### Analysis

The stacking pipeline in `stack_images_direct` (line 230) processes images one by one in a for-loop. For each image:
1. Upload to GPU ‚Üí convert to float ‚Üí split BGR/Alpha ‚Üí generate Laplacian pyramid ‚Üí compute energy ‚Üí fuse with running result

Steps 1-4 are **independent per image** until the fusion step. This means Laplacian pyramid generation for all N images could run in parallel, then fusion runs sequentially.

### Optimizations

| ID | Change | Impact | Effort | Risk |
|----|--------|--------|--------|------|
| **K1** | **Parallel pyramid generation** ‚Äî Pre-generate Laplacian pyramids for ALL images in the batch using Rayon, then fuse sequentially. Pyramid gen (pyr_down √ó 7 levels) is the most expensive step and is independent per image. | üî• High ‚Äî pyramid gen dominates stacking time | Medium | Medium ‚Äî GPU memory for N pyramids simultaneously |
| **K2** | **Parallel layer fusion** ‚Äî After generating all pyramids, fuse each pyramid level independently in parallel (7 levels = 7 threads). Each level's fusion is independent of other levels. | Medium ‚Äî 7-way parallelism for fusion | Medium | Low |
| **K3** | **Pre-split BGR/Alpha during parallel load** ‚Äî `extract_bgr_and_alpha` (line 247) is called per-image inside the sequential loop. Move it to the parallel image loading phase. | Low ‚Äî small savings per image | Easy | None |
| **K4** | **Reduce GPU‚ÜîCPU transfers** ‚Äî Final conversion (line 515-518) does `convert_to(CV_8U)` on UMat ‚Üí `get_mat` ‚Üí `copy_to`. Could skip the final copy_to. | Low ‚Äî one less copy | Easy | None |

---

## Priority Matrix

### ü•á Tier 1 ‚Äî High Impact, Easy Effort (Do First)

| ID | Area | Change | Expected Speedup |
|----|------|--------|-----------------|
| **S1** | Sharpness | Remove OPENCL_MUTEX from sharpness handler | 2-4x (N threads truly parallel) |
| **A1** | Alignment (ORB/SIFT/AKAZE) | Remove OPENCL_MUTEX from GPU preprocessing | 2-4x for preprocessing phase |
| **A2** | Alignment (ORB/SIFT/AKAZE) | Remove OPENCL_MUTEX from warping | 2-4x for warping phase |
| **H1** | Alignment (Hybrid) | Remove OPENCL_MUTEX from keypoint extraction | Minor ‚Äî removes unnecessary serialization |

### ü•à Tier 2 ‚Äî High Impact, Medium Effort

| ID | Area | Change | Expected Speedup |
|----|------|--------|-----------------|
| **K1** | Stacking | Parallel pyramid generation | 2-3x for stacking phase |
| **A3** | Alignment (ORB/SIFT/AKAZE) | Use UMat for warping instead of CPU Mat | 2-5x for warping 42MP images |
| **E1** | Alignment (ECC) | Use UMat for warping | 2-5x for ECC warping |
| **E2** | Alignment (ECC) | Parallel sharpness pre-filter (add par_iter) | 2-4x for ECC sharpness phase |

### ü•â Tier 3 ‚Äî Lower Impact / Diminishing Returns

| ID | Area | Change | Expected Speedup |
|----|------|--------|-----------------|
| **T1** | Thumbnails | CPU-only resize (remove UMat overhead) | 10-30% per thumbnail |
| **K2** | Stacking | Parallel layer fusion (7 levels) | 1.5-2x for fusion phase |
| **A4** | Alignment | Parallel pairwise matching | Minor for matching phase |
| **S2** | Sharpness | Parallel regional grid computation | 1.5x per image |
| **S3** | Sharpness | Eliminate redundant global sharpness call | Minor |
| **T2** | Thumbnails | Progressive thumbnails (fast preview) | Perceived speed only |
| **E3** | Alignment (ECC) | GPU preprocessing for ECC | Medium for large images |
| **K3** | Stacking | Pre-split BGR/Alpha in parallel load | Minor |

---

## Implementation Notes

### Removing OPENCL_MUTEX (S1, A1, A2, H1)

The safest approach:

1. **Remove the mutex calls** ‚Äî delete `let _lock = opencl_mutex().lock().unwrap();` and the corresponding `drop(_lock);`
2. **Add env var fallback** ‚Äî like ECC already does: `IMAGESTACKER_OPENCL_MUTEX=1` to re-enable if crashes occur
3. **Test incrementally** ‚Äî remove one mutex at a time, test with 46√ó42MP images
4. **Keep the `opencl_mutex()` function** ‚Äî it's still used as an optional safety net

### GPU Warping (A3, E1)

Replace the CPU warp pattern:
```rust
// Current (CPU):
let mut warped = Mat::default();
imgproc::warp_affine(&img, &mut warped, &transform, size, ...)?;

// Optimized (GPU):
let img_umat = img.get_umat(AccessFlag::ACCESS_READ, UMatUsageFlags::USAGE_DEFAULT)?;
let mut warped_umat = UMat::new(UMatUsageFlags::USAGE_DEFAULT);
imgproc::warp_affine(&img_umat, &mut warped_umat, &transform, size, ...)?;
let warped = warped_umat.get_mat(AccessFlag::ACCESS_READ)?;
```

### Parallel Pyramid Generation (K1)

```rust
// Current (sequential):
for (idx, img) in images.iter().enumerate() {
    let pyramid = generate_laplacian_pyramid(&bgr, levels)?;
    // ... fuse immediately ...
}

// Optimized (parallel gen, sequential fuse):
let pyramids: Vec<_> = images.par_iter()
    .map(|img| {
        let float_img = /* upload + convert */;
        let (bgr, alpha) = extract_bgr_and_alpha(&float_img)?;
        let pyramid = generate_laplacian_pyramid(&bgr, levels)?;
        let energies: Vec<_> = pyramid.iter()
            .map(|layer| compute_sharpness_energy(layer))
            .collect::<Result<_>>()?;
        Ok((pyramid, energies, alpha))
    })
    .collect::<Result<Vec<_>>>()?;

// Sequential fusion (depends on running result)
for (pyramid, energies, alpha) in pyramids { ... }
```

**Memory concern:** Each pyramid for a 42MP image ‚âà 500-800MB on GPU. For a batch of 6 images, that's 3-5GB of GPU memory. May need to limit parallel pyramid count based on available GPU memory.

---

## Measurement Plan

Before implementing, measure current performance baselines:

```bash
# Measure sharpness detection time
time RUST_LOG=info cargo run --release -- -i testimages_small/ 2>&1 | grep "Sharpness detection complete"

# Measure alignment time (ORB)
time RUST_LOG=info cargo run --release -- -i testimages/ 2>&1 | grep "Alignment completed"

# Measure stacking time
time RUST_LOG=info cargo run --release -- -i testimages/ 2>&1 | grep "Stacking.*complete"
```

After each optimization, re-measure and compare. Expected total improvement from Tier 1 changes alone: **30-60% faster** end-to-end for a typical 46-image workflow.
